<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-07-10T13:15:50-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">aimless agents</title><subtitle>just 2 agents looking to learn</subtitle><entry><title type="html">Cross Entropy and Kullback-Leibler</title><link href="http://localhost:4000/articles/2021-03/cekl" rel="alternate" type="text/html" title="Cross Entropy and Kullback-Leibler" /><published>2021-03-18T00:00:00-04:00</published><updated>2021-03-18T00:00:00-04:00</updated><id>http://localhost:4000/articles/2021-03/cekl</id><content type="html" xml:base="http://localhost:4000/articles/2021-03/cekl">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;We wanted to dedicate an entire post to the lovely functions cross entropy and Kullback-Leibler divergence, which are very widely used in training ML models but not very intuitive.&lt;/p&gt;

&lt;p&gt;Luckily these two loss functions are intricately related, and in this post we’ll explore the intuitive ideas behind both, and compare &amp;amp; contrast the two so you can decide which is more appropriate to use in your case. We’ve also created a short interactive demo you can play around with at the &lt;a href=&quot;#demo&quot;&gt;bottom of the post&lt;/a&gt;.
&lt;!-- (By the way, for a more bird’s eye view of these two as well as loss function as a whole, check out our Loss Discussion post here!) --&gt;&lt;/p&gt;

&lt;h2 id=&quot;entropy&quot;&gt;Entropy&lt;/h2&gt;

&lt;p&gt;We’ll start off reviewing the concepts of &lt;strong&gt;information&lt;/strong&gt; and &lt;strong&gt;entropy&lt;/strong&gt;. 
“Information” is such an overloaded word in English, but in the statistical/information-theoretic sense, information is a quantification of &lt;strong&gt;uncertainty&lt;/strong&gt; in a probability distribution. To illustrate, consider the weather. The more &lt;em&gt;uncertain&lt;/em&gt; or random the weather seems, the more information you gain when you learn what the weather actually is.
Entropy is centered around this information transfer: it’s the expected amount of information, measured in bits, in some transmission of data.&lt;/p&gt;
&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://media4.giphy.com/media/3otPoG6dAdT9dQfdZu/giphy.gif&quot; alt=&quot;Unfortunately Karen provides pretty low information.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Unfortunately Karen provides pretty low information.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For example: suppose the weather can either be sunny or rainy. 
Now suppose the true distribution of the weather is P(sunny) = ¾, P(rainy) = ¼. We could then say &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10&lt;/code&gt; mean sunny, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;11&lt;/code&gt; means rainy. This is known as an &lt;strong&gt;encoding&lt;/strong&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-sun&quot; style=&quot;font-size: 60px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;00&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-sun&quot; style=&quot;font-size: 60px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;01&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-sun&quot; style=&quot;font-size: 60px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;10&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-cloud-showers-heavy&quot; style=&quot;font-size: 60px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;11&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
    &lt;/div&gt;
&lt;/figure&gt;

&lt;p&gt;As a general rule, the information (in bits) needed to encode an outcome = \(-log_2( P(outcome) )\). Why? A singular bit can take on 2 unique values; so, if you imagine all the possible weather outcomes represented in a tree, each individual bit helps split a path in the tree in two.&lt;/p&gt;
&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/ce-kl/weather_tree.png&quot; alt=&quot;&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The more rare the outcome, the more information you’ll need to discern that event from others – hence the &lt;strong&gt;inverse relationship&lt;/strong&gt; between probability and information for a given event. 
Equivalently to the above,&lt;/p&gt;

\[P(event) = (\frac{1}{2})^{\# bits}.\]

&lt;p&gt;To further illustrate this formula, let’s consider the 2 possible outcomes from learning what the weather is:&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;The weather station tells us it’s rainy.&lt;br /&gt;&lt;br /&gt;&lt;i class=&quot;fa fa-sun sm-icon&quot; style=&quot;color:gray;&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-sun sm-icon&quot; style=&quot;color:gray&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-sun sm-icon&quot; style=&quot;color:gray&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-cloud-showers-heavy sm-icon&quot;&gt;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;We got \(-log_2(1/4)\) = 2 bits of info. The station must’ve transmitted &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;11&lt;/code&gt;, which is precisely 2 bits long.&lt;/td&gt;
      &lt;td&gt;The weather station tells us it’s sunny.&lt;br /&gt;&lt;br /&gt;&lt;i class=&quot;fa fa-sun sm-icon&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-sun sm-icon&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-sun sm-icon&quot;&gt;&lt;/i&gt; &lt;i class=&quot;fa fa-cloud-showers-heavy sm-icon&quot; style=&quot;color:gray&quot;&gt;&lt;/i&gt;&lt;br /&gt;&lt;br /&gt;We got \(-log_2(3/4)\) = 0.415 bits of info. This is a bit trickier than the rainy case: &lt;br /&gt;the station transmitted one of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;00&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;01&lt;/code&gt;, or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;10&lt;/code&gt;, which is less than 2 bits of info because we’re not certain what each bit was.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now to calculate the &lt;em&gt;expected amount of information&lt;/em&gt; (entropy!), we take the weighted sum of these individual amounts of information, with weights being the probability of each outcome.&lt;/p&gt;

\[Entropy(weather) = H(weather) = E[Info(weather)] = P(rainy) * Info(rainy) + P(sunny) * Info(sunny)\]

\[= -0.25 * log_2(0.25) - 0.75 * log_2(0.75) = 0.811.\]

&lt;p&gt;This tells us that given the weather follows our distribution of ¾ sunny, ¼ rainy, we’d expect that learning what the weather is for a given day (i.e., receiving the information about the weather’s value) gives us 0.811 bits’ worth of information. Or, from the other side, we expect it would take 0.811 bits to send a message saying what today’s weather was.
This brings us to the general formula for entropy, which is defined for some probability distribution (in this case, of the weather) \(p\):&lt;/p&gt;

\[H(p) = - \sum_i p_i log_2(p_i)\]

&lt;h2 id=&quot;categorical-cross-entropy&quot;&gt;(Categorical) Cross Entropy&lt;/h2&gt;

&lt;p&gt;Let’s get to the main attraction: &lt;strong&gt;cross entropy&lt;/strong&gt; is the expected number of bits needed to encode data from one distribution using another. So this is where our model’s prediction of the weather comes in. Maybe we live in a temperate place, so we think P(sunny) = 9/10. With our &lt;em&gt;predicted&lt;/em&gt; distribution, this would require us to encode a “sunny” outcome with \(-log_2(9/10)\) = 0.15 bits. Since the &lt;em&gt;actual&lt;/em&gt; probability it’s sunny is still ¾, the expected amount of information is ¾ * 0.15. So again taking the weighted sum in this manner, the formula for cross entropy loss is&lt;/p&gt;

\[H(p, q) = - \sum_i p_i log_2(q_i)\]

\[= ¾ (0.15) + ¼ (3.32)\]

&lt;p&gt;Again, this represents the average number of bits needed to encode data from a source with distribution \(p\), using a model distribution \(q\). Why does this quantity of information/number of bits matter? Intuitively, the less accurate or further away \(q\) is from \(p\), the more bits are needed to represent \(p\).&lt;/p&gt;

&lt;p&gt;When training, since we don’t know the true distribution, but instead the actual classes (labels) for individual data points, we compute the CE loss between our predicted distribution and &lt;strong&gt;one-hot vectors&lt;/strong&gt; for each training point. This means a distribution with probability of 1 on the true label class, and 0 for all other classes.&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/ce-kl/one_hot.png&quot; alt=&quot;One hot arrow with both direction and magnitude.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;One hot arrow with both direction and magnitude.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;For example, if our model outputted this the probability distribution&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-sun&quot; style=&quot;font-size: 35px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;0.75&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-cloud-showers-heavy&quot; style=&quot;font-size: 35px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;0.25&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
    &lt;/div&gt;
&lt;/figure&gt;

&lt;p&gt;But the correct answer was sunny, making the one-hot vector:&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-sun&quot; style=&quot;font-size: 35px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;1&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
            
            &lt;div style=&quot;margin: 0 10px&quot;&gt;
                &lt;i class=&quot;fa fa-cloud-showers-heavy&quot; style=&quot;font-size: 35px; color: ;&quot;&gt;&lt;/i&gt;
                &lt;p style=&quot;font-size: 20px; margin-top: 0px&quot;&gt;0&lt;/p&gt;
            &lt;/div&gt;
            &lt;!-- &lt;img class=&quot;image&quot; src=&quot;&quot; alt=&quot;&quot;&gt; --&gt;
        
    &lt;/div&gt;
&lt;/figure&gt;

&lt;p&gt;Then the cross entropy loss is accordingly&lt;/p&gt;

\[- 1 * log_2(0.75) - 0 * log_2(0.25)\]

&lt;p&gt;Had our model predicted it was sunny with probability 1, the cross entropy loss for this example would’ve been 0. &lt;em&gt;(Think about why!)&lt;/em&gt;&lt;/p&gt;
&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/ce-kl/always_sunny.png&quot; alt=&quot;P(sunny) = 1 in Philadelphia!&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;P(sunny) = 1 in Philadelphia!&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;One property of the cross entropy formula is that if you predict a class has probability of 0 when its true probability is non-zero, the cross entropy loss will be infinite/undefined. This is because \(log(x)\) approaches positive infinity as x approaches 0. 
In practice, since we can’t backpropagate from a loss value of infinity, two solutions to this problem are&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Clip the predicted probability values at some small \(\epsilon\) value, e.g., 1e-6, which would result in a large but at least defined value.&lt;/li&gt;
  &lt;li&gt;Clip the cross entropy loss at some large value, e.g., \(loss(p, q) = min(1000, H(p, q))\), so it never actually reaches infinity.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;It’s worth noting that despite being referred to as a “distance”, cross entropy is not symmetric: \(H(p, q) \ne H(q, p)\).&lt;/p&gt;

&lt;p&gt;Cross entropy is a great loss function to use for most &lt;strong&gt;multi-class classification&lt;/strong&gt; problems. This describes problems like our weather-predicting example: you have 2+ different classes you’d like to predict, and each example only belong to &lt;em&gt;one&lt;/em&gt; class.&lt;/p&gt;

&lt;p&gt;The cross entropy we’ve defined in this section is specifically &lt;strong&gt;categorical cross entropy&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;binary-cross-entropy-log-loss&quot;&gt;Binary cross-entropy (log loss)&lt;/h3&gt;

&lt;p&gt;For &lt;strong&gt;binary classification&lt;/strong&gt; problems (when there are only 2 classes to predict) specifically, we have an alternative definition of CE loss which becomes binary CE (BCE) loss. This is commonly referred to as log loss:
\(BCE(p, q) = - \sum_i p_i log(q_i) + (1 - p_i) log(1 - q_i)\)&lt;/p&gt;

&lt;p&gt;You’ll notice that this equation has a few terms extra compared to categorical cross entropy. The \(p_i log(q_i)\) term is familiar, measuring the distance between true and predicted class probability, while the \((1 - p_i) log(1 - q_i)\) is basically the exact same penalty for the other side: we know there’s exactly 2 classes, so \((1 - p_i)\) and \((1-q_i)\) are just the probabilities for the opposite class to \(p_i\) and \(q_i\), respectively.&lt;/p&gt;

&lt;p&gt;So when to use this instead of the categorical cross entropy loss covered above? As you may guess, this loss function is only appropriate for binary classification. Another use case would be &lt;strong&gt;multi-label classification&lt;/strong&gt;, which means assigning multiple classes (“labels”) to each data point. This is essentially combining multiple binary classification tasks into one, for example, asking for a given day:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;is it rainy or not rainy?&lt;/li&gt;
  &lt;li&gt;is it sunny or not sunny?&lt;/li&gt;
  &lt;li&gt;is it cloudy or not cloudy?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Where the difference between this and multi-class classification is that with the former, a day can be labeled with multiple classes (e.g., rainy, not sunny, and cloudy), but for multi-class classification, only one would hold since being in one class implies not being in any other.&lt;/p&gt;

&lt;h2 id=&quot;kullback-leibler-kl-divergence&quot;&gt;Kullback-Leibler (KL) divergence&lt;/h2&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/ce-kl/ohyeah.jpg&quot; alt=&quot;Cross entropy and entropy come together to make KL divergence.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Cross entropy and entropy come together to make KL divergence.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;KL divergence is just the difference between cross entropy and the true distribution’s entropy! 
\(KL(p, q) = H(p, q) - H(p)\)
Rearranging terms and using log properties, we get an alternate (but more commonly seen) formulation (note that the || just signifies “distance between” in statistics):
\(KL(p || q) = H(p, q) - H(p) = - \sum_i p_i log_2(q_i) - \sum_i p_i log_2(p_i) = - \sum_i p_i (log_2(q_i) - log_2(p_i)) = \sum_i p_i log(\frac{p_i}{q_i})\)
which is sometimes re-expressed as:
\(E[log \frac{p_i}{q_i}] = E[log p_i - log q_i]\)&lt;/p&gt;

&lt;p&gt;Intuitively, it can be thought of as the expected number of extra bits needed to encode outcomes drawn from the true distribution \(p\) if we’re using distribution \(q\), or “the measure of inefficiency in using the probability distribution q to approximate the true probability distribution p” (thanks Wikipedia!).
If we have a perfect prediction, i.e., our predicted distribution equals the true, then cross entropy equals the true distribution’s entropy, making KL divergence 0 (its minimum value).&lt;/p&gt;

&lt;p&gt;KL divergence is used with &lt;strong&gt;generative models&lt;/strong&gt;, for example, variational autoencoders (VAEs) or generative adversarial networks (GANs). At a high level, generative models generate data that follows a similar distribution as the training data (for example, https://thesecatsdonotexist.com/). KL divergence then functions as a “distance” metric between predicted and reference distribution.&lt;/p&gt;

&lt;p&gt;See one of these awesome resources to read more:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;https://www.jeremyjordan.me/variational-autoencoders/&lt;/li&gt;
  &lt;li&gt;https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;compare-n-contrast&quot;&gt;Compare n Contrast&lt;/h2&gt;

&lt;p&gt;Because the \(H(p)\) term in KL divergence doesn’t change with the predicted distribution, minimizing cross entropy (the \(H(p, q)\) term) also minimizes KL divergence. So if it seems like we’re optimizing our model in the same way using both loss functions, does it even matter which one we use?&lt;/p&gt;

&lt;p&gt;KL divergence exactly equals cross entropy when the entropy of the true distribution is 0. This happens when there is no randomness in the distribution, i.e., one outcome/class has probability 1. Notice that this is precisely the case with how we train models, since we use a one-hot vector for the correct class. So when training a classifier for which you have some set of labelled training points you’d like your model to generalize over, the two are functionally equivalent, and you may as well just use cross entropy loss.&lt;/p&gt;

&lt;p&gt;Meanwhile, generative models try to output a probability distribution, so KL divergence makes more sense.&lt;/p&gt;

&lt;p&gt;So the bottom line: cross entropy is for training a classifier, KL divergence for a generator.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Loss function&lt;/th&gt;
      &lt;th&gt;Use case(s)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Categorical cross entropy&lt;/td&gt;
      &lt;td&gt;- multi-class classification&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Binary cross entropy&lt;/td&gt;
      &lt;td&gt;- binary classification &lt;br /&gt; - multi-label classification&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;KL divergence&lt;/td&gt;
      &lt;td&gt;- outputting a distribution&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;demo&quot;&gt;Demo&lt;/h2&gt;

&lt;p&gt;Below is the cross entropy + KL divergence demo, which uses our recurring weather classification problem. Click &lt;a href=&quot;/ce-demo/index.html&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; for a full-screen version if preferred.&lt;/p&gt;

&lt;p&gt;The blue text superimposed over these icons shows the true weather probability distribution, while the values shown below the classes in green are the predictions, which you can control using the sliders. As you adjust these values, you’ll see the 3 values at the bottom of the screen - entropy, cross entropy, and KL, respectively - change accordingly.&lt;/p&gt;

&lt;p&gt;To the left of the screen is a collapsible menu. Here you can add and remove classes, up to 10 and down to 1, and change the true distribution by randomizing their values (with either a totally random distribution, or a random one-hot vector), or by manually setting the values yourself.&lt;/p&gt;

&lt;p&gt;Note that this demo doesn’t leverage the “clipping” tricks mentioned in &lt;a href=&quot;#categorical-cross-entropy&quot;&gt;the section above&lt;/a&gt;, so setting the predicted probability to 0 for a class with non-zero true probability will result in a cross-entropy (and correspondingly, KL) of infinity!&lt;/p&gt;

&lt;iframe src=&quot;/ce-demo/index.html&quot; title=&quot;Cross Entropy loss demo&quot; width=&quot;100%&quot; height=&quot;565px&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;demo-walkthrough&quot;&gt;Demo walkthrough&lt;/h2&gt;

&lt;p&gt;If you want some more guidance using the demo, click/tap the question mark button in the upper right corner, then “Play Tutorial”. This will walk you through the different things you can do with the demo and explain some of the things you see on the screen.
This demo is optimized for desktop use, so if you’re reading this on a phone, I’d recommend playing with the demo on a desktop!&lt;/p&gt;

&lt;p&gt;Let’s walk through a simple example. Make sure you have 4 classes, then click “Set” to set the true distribution to the uniform distribution (all classes have the same probability). Then set all the sliders to the same value too. Because p = q exactly, you should see that cross entropy and entropy are exactly the same. This in turn brings KL divergence down to 0, its smallest possible value. Cross entropy is also at its lowest possible value for the given problem. 
Now drag some sliders to change the predicted distribution to something not uniform. The farther you change it, the higher KL divergence and cross entropy will get.
Now try putting your loss knowledge to the test. Toggle “Show true distribution” to hide the true distribution, then randomize it. Try to guess the true distribution by adjusting the sliders. That’s somewhat similar to how your model learns during training!&lt;/p&gt;</content><author><name></name></author><category term="loss" /><category term="demo" /><category term="explanation" /><summary type="html">A beginner&apos;s guide to cross entropy loss and KL divergence.</summary></entry><entry><title type="html">Hindsight is 2020</title><link href="http://localhost:4000/articles/2021-02/HER" rel="alternate" type="text/html" title="Hindsight is 2020" /><published>2021-02-11T00:00:00-05:00</published><updated>2021-02-11T00:00:00-05:00</updated><id>http://localhost:4000/articles/2021-02/HER</id><content type="html" xml:base="http://localhost:4000/articles/2021-02/HER">&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#what-is-td3&quot; id=&quot;markdown-toc-what-is-td3&quot;&gt;What is TD3?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#what-is-her&quot; id=&quot;markdown-toc-what-is-her&quot;&gt;What is HER?&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#why-td3--her&quot; id=&quot;markdown-toc-why-td3--her&quot;&gt;Why TD3 + HER?&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#setup&quot; id=&quot;markdown-toc-setup&quot;&gt;Setup&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fetchreach-environment&quot; id=&quot;markdown-toc-fetchreach-environment&quot;&gt;FetchReach Environment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sparsified-reacher-environment&quot; id=&quot;markdown-toc-sparsified-reacher-environment&quot;&gt;Sparsified Reacher Environment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#results&quot; id=&quot;markdown-toc-results&quot;&gt;Results&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#fetchreach-environment-results&quot; id=&quot;markdown-toc-fetchreach-environment-results&quot;&gt;FetchReach Environment Results&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sparsified-reacher-environment-results&quot; id=&quot;markdown-toc-sparsified-reacher-environment-results&quot;&gt;Sparsified Reacher Environment Results&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#takeaways&quot; id=&quot;markdown-toc-takeaways&quot;&gt;Takeaways&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#resources&quot; id=&quot;markdown-toc-resources&quot;&gt;Resources&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#papers&quot; id=&quot;markdown-toc-papers&quot;&gt;Papers&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#other-resources&quot; id=&quot;markdown-toc-other-resources&quot;&gt;Other Resources&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#appendix&quot; id=&quot;markdown-toc-appendix&quot;&gt;Appendix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This past semester we took a &lt;a href=&quot;https://cmudeeprl.github.io/403_website/&quot; target=&quot;_blank&quot;&gt;deep reinforcement learning course&lt;/a&gt; together at CMU. The class introduced us to goal-conditioned learning and  &lt;strong&gt;Hindsight Experience Replay (HER)&lt;/strong&gt;. The underlying concepts behind HER interested us, and we wanted to try reproducing the authors’ results in sparse/binary reward environments - in our case, simulated reacher environments - benchmarked against vanilla TD3. We hope to illustrate the benefits of using hindsight experience in sparse, binary reward environments through sharing a discussion of &lt;a href=&quot;https://github.com/aimless-agents/TD3&quot; target=&quot;_blank&quot;&gt;our implementation&lt;/a&gt;, experimentation, and methods. While we won’t go in-depth into the implementation details, we hope that you come out of this with a better idea of why hindsight replay is so useful and interesting, and how we went about trying to prove it!&lt;/p&gt;

&lt;h3 id=&quot;what-is-td3&quot;&gt;What is TD3?&lt;/h3&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/td3_spiderman.png&quot; alt=&quot;An illustration of how TD3 works.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;An illustration of how TD3 works.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.09477&quot; target=&quot;_blank&quot;&gt;TD3&lt;/a&gt; is an off-policy RL method for continuous action spaces, which improves its predecessor, &lt;a href=&quot;https://arxiv.org/abs/1509.02971&quot; target=&quot;_blank&quot;&gt;DDPG&lt;/a&gt;, by reducing overestimation bias and improving training stability. This generally results in higher performance than DDPG. We’re going to assume we all have a working knowledge of RL algorithms, but if you want to learn more then you can read more details about TD3 and DDPG &lt;a href=&quot;https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;what-is-her&quot;&gt;What is HER?&lt;/h3&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;https://i.pinimg.com/originals/b7/f6/f4/b7f6f409e68715245296a33ef5452b3c.gif&quot; alt=&quot;Some good questions from our good man Michael Bluth.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Some good questions from our good man Michael Bluth.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;To understand &lt;a href=&quot;https://arxiv.org/abs/1707.01495&quot; target=&quot;_blank&quot;&gt;HER&lt;/a&gt;, you’ll first need to understand &lt;strong&gt;regular experience replay (ER)&lt;/strong&gt;. ER involves learning from an agent’s memory, by storing past experiences (more specifically, transitions) in a replay buffer. Then, the agent trains on randomly sampled transitions from the replay buffer by “replaying” these transitions. Training on randomly sampled transitions rather than sequentially collected experience helps de-correlate training data and improves learning. You can learn more about it &lt;a href=&quot;https://www.youtube.com/watch?v=Bcuj2fTH4_4&amp;amp;t=138s&amp;amp;ab_channel=deeplizard&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goal-conditioned learning&lt;/strong&gt; generally means learning to reach a goal &lt;em&gt;in addition to&lt;/em&gt; maximizing reward. Let’s illustrate this with a motivating example:&lt;/p&gt;

&lt;p&gt;A soccer player successfully kicks the ball into the goal - this is obviously a helpful learning example, as the player learns to repeat their sequence of actions to achieve their goal. But what about if the ball lands, say, a foot away from the goalpost? We can “move” the goalpost to wherever the ball actually ended up, so the player learns to celebrate making goals rather than feeling sad that they did not make the original goal. This helps the soccer player learn how to score better overall.&lt;/p&gt;

&lt;p&gt;HER leverages this concept by augmenting real experience collected for the replay buffer with hindsight about states the agent was actually able to reach. For example, let’s say Turtle Claire is trying to make a goal. If the goal is 3 cells away, traditional goal-conditioned experience replay would tell her to store each of these transitions in her replay buffer with the original goal state at cell 3 and no return (AKA a cumulative reward of 0), since she did not score a goal.&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/sadsoccerturtle.png&quot; alt=&quot;Turtle Claire tried to score a goal but didn&apos;t make it. This makes her feel sad.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Turtle Claire tried to score a goal but didn&apos;t make it. This makes her feel sad.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;But with HER, Turtle Claire would also store each of these transitions with a goal state of cell 2 and positive return, since she would have scored had the goal been in cell 2. Even though this is not the original goal we wanted to achieve, we are still able to learn something useful by “moving the goalposts” in hindsight, which is the crux of HER.&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/happysoccerturtle.png&quot; alt=&quot;In hindsight, Turtle Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;In hindsight, Turtle Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;HER and goal-conditioned methods aim to solve the issue of learning in an environment with &lt;strong&gt;sparse rewards&lt;/strong&gt;, which means very few states/actions in the environment actually give a positive or non-zero reward signal. Often, these sparse rewards are &lt;strong&gt;binary&lt;/strong&gt;, as in Turtle Claire’s soccer scenario: you have either scored a goal, or you have not.&lt;/p&gt;

&lt;p&gt;HER also helps with &lt;strong&gt;sample efficiency&lt;/strong&gt;: since we augment our experience buffer with these goal-conditioned examples for replay, we have access to far more training data while using the same amount of examples drawn from the environment.&lt;/p&gt;

&lt;h3 id=&quot;why-td3--her&quot;&gt;Why TD3 + HER?&lt;/h3&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/td3hermovie.png&quot; alt=&quot;A romance for the epochs.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;A romance for the epochs.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;If you read the HER paper, you’ll notice that the authors implemented HER with DDPG. We were wondering why they didn’t use TD3 instead, given that it’s a marked improvement on DDPG. After some digging, we found out that the HER paper came out one year before Fujimoto published his paper introducing TD3, which was a nice reminder of how quickly the field moves!&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/sotagrandma.png&quot; alt=&quot;It can be difficult to keep up sometimes...&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;It can be difficult to keep up sometimes...&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;At any rate, poking around on &lt;a href=&quot;https://paperswithcode.com/paper/hindsight-experience-replay&quot; target=&quot;_blank&quot;&gt;Papers With Code&lt;/a&gt; and Google yielded re-implementations of the DDPG + HER results from the original paper. Although we did find some implementations of TD3+HER out there (like &lt;a href=&quot;https://arxiv.org/pdf/2010.06142.pdf&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;https://journals.sagepub.com/doi/full/10.1177/1729881419898342&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt;, and &lt;a href=&quot;https://github.com/hill-a/stable-baselines&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt;), our curiosity was sufficiently piqued by TD3 and HER that we wanted to do our own exploration of it.&lt;/p&gt;

&lt;p&gt;Specifically, we wanted to see what would happen if we trained our agents using TD3 with HER, instead of standard ER (which TD3 was originally proposed with). Even though TD3 tends to perform better than DDPG by improving stability and reducing overestimation bias during training, we expect that it won’t be able to overcome difficulties with learning in sparse reward environments. This is where HER comes in.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;We started with Fujimoto’s &lt;a href=&quot;https://github.com/sfujim/TD3&quot; target=&quot;_blank&quot;&gt;original TD3 implementation&lt;/a&gt; and added hindsight replay functionality on top of it. For more details, you can see our code &lt;a href=&quot;https://github.com/clairecw/TD3&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our goal was to train the agent in two different variations of a sparse-reward environment: OpenAI’s &lt;a href=&quot;https://gym.openai.com/envs/FetchReach-v1/&quot; target=&quot;_blank&quot;&gt;FetchReach environment&lt;/a&gt; (designed for learning sparse reward), and a sparsification of OpenAI’s &lt;a href=&quot;https://gym.openai.com/envs/Reacher-v2/&quot; target=&quot;_blank&quot;&gt; MuJoCo Reacher environment&lt;/a&gt; (a dense reward environment that we adapted into a sparse reward environment).&lt;/p&gt;

&lt;p&gt;For both environments, we evaluated the corresponding robot arm’s performance trained on:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;vanilla TD3 in a sparse reward environment, which we will refer to as the &lt;strong&gt;&lt;em&gt;Sparse TD3&lt;/em&gt;&lt;/strong&gt; agent&lt;/li&gt;
  &lt;li&gt;TD3 + HER in a sparse reward environment, which we will refer to as the &lt;strong&gt;&lt;em&gt;TD3+HER&lt;/em&gt;&lt;/strong&gt; agent&lt;/li&gt;
  &lt;li&gt;vanilla TD3 in a dense reward environment, which we will refer to as the &lt;strong&gt;&lt;em&gt;Dense TD3&lt;/em&gt;&lt;/strong&gt; agent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We will describe the environments in more detail below.&lt;/p&gt;

&lt;h3 id=&quot;fetchreach-environment&quot;&gt;FetchReach Environment&lt;/h3&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/fetchreach_random.gif&quot; alt=&quot;Agent in FetchReach environment taking random actions.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Agent in FetchReach environment taking random actions.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;There is an &lt;a href=&quot;https://openai.com/blog/ingredients-for-robotics-research/&quot; target=&quot;_blank&quot;&gt;awesome suite of sparse reward environments&lt;/a&gt; designed by &lt;a href=&quot;https://openai.com/&quot; target=&quot;_blank&quot;&gt;OpenAI&lt;/a&gt; that work with HER already that were benchmarked on &lt;a href=&quot;https://arxiv.org/abs/1802.09464&quot; target=&quot;_blank&quot;&gt;DDPG + HER&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This environment is arguably the simplest of this suite of sparse reward environments that OpenAI released. The goal of this environment is to control a 3 degree-of-freedom robotic arm to reach a block placed before it in 3D space. The block’s location is fixed throughout the &lt;strong&gt;episode&lt;/strong&gt; (where an “episode” is one attempt to reach the goal, capped out at 50 timesteps), but is randomly generated at the start of each episode.&lt;/p&gt;

&lt;p&gt;This environment is default-set to return a sparse reward: it will give a reward \(r_t\) of 0 at timestep \(t\) if the block is within \(\epsilon\) of goal state \(g\) after taking action \(a\) from state \(s\), and 0 reward if it is not:&lt;/p&gt;

\[r_t(s, a, g) = \left\{\begin{array}{ll}
0 &amp;amp; d(s, g) &amp;lt; \epsilon\\
-1 &amp;amp; \text{otherwise}
\end{array}
\right.\]

&lt;p&gt;The &lt;strong&gt;return&lt;/strong&gt; \(R\) of one episode in the FetchReach environment is the cumulative reward over each timestep in the episode:&lt;/p&gt;

\[R = \sum_t r_t\]

&lt;p&gt;How do these formulas correspond to getting the robot arm to reach as close to the target block as possible? If we received an episode return \(R = -50\), this would imply that our robot arm was  not within \(\epsilon\) of the block, or any goal state, throughout the entire episode. That’s not what we want! On the other hand, if we received an episode return \(R = 0\), this means that at every timestep in our episode, the robot arm was within \(\epsilon\) of the block, or any goal state, throughout the entire episode. This &lt;em&gt;is&lt;/em&gt; what we want! Therefore, we want to train our TD3 agent to maximize cumulative reward and achieve close to 0 episode return.&lt;/p&gt;

&lt;h3 id=&quot;sparsified-reacher-environment&quot;&gt;Sparsified Reacher Environment&lt;/h3&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/crrandom.gif&quot; alt=&quot;Agent in Reacher environment taking random actions.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Agent in Reacher environment taking random actions.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The Reacher-v2 OpenAI Gym environment is similar to FetchReach in that a robot arm must reach a goal object. However, it has a shaped reward function that includes distance to the goal object, as well as a reward penalty for extraneous robot arm movement, and only has two degrees of freedom. In addition, this environment is considered “solved” (i.e. the robot arm has reached the goal object) when the episode return is greater than -3.75.&lt;/p&gt;

&lt;p&gt;OpenAI’s FetchReach environment was made &lt;em&gt;specifically&lt;/em&gt; to benchmark goal-conditioned learning strategies, so we wanted to challenge our implementation to see if it could still solve an environment that wasn’t “born” to be sparse. Therefore, we figured that the Reacher-v2 OpenAI Gym environment, which has a similar goal to the FetchReach environment but was designed with a shaped reward signal in mind, would be a good candidate for sparsification. Therefore, we extended the MuJoCo Reacher environment in order to convert it to a sparse reward environment.&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/borntobesparse.png&quot; alt=&quot;Some environments were born to be sparse.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;Some environments were born to be sparse.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;p&gt;The main changes we made to the original Bullet environment were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sparsifying the environment:&lt;/strong&gt; the agent only gets a reward if it’s within some distance \(\epsilon\) of the target location, otherwise 0.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Augment the existing standard replay buffer with the agent’s intended goal and goal-conditioned rewards.&lt;/strong&gt; We used the HER authors’ “future” goal-choosing scheme, which means after each episode, a state \(s\) selected for hindsight replay would be stored with its goal being a uniform-randomly selected state that occurred &lt;em&gt;after&lt;/em&gt; \(s\) in the episode.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some prominent variations to the reward definition and HER training we tried were:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Varying \(\epsilon\), or how “off” the agent can be from the goal and still receive reward for “achieving” the goal.&lt;/li&gt;
  &lt;li&gt;Varying \(k\), the ratio of standard ER and HER-generated memory to store for replay.&lt;/li&gt;
  &lt;li&gt;Including/excluding factors other than distance to goal in the reward function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See the Appendix for a full description of these parameters, along with additional things we varied.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;h3 id=&quot;fetchreach-environment-results&quot;&gt;FetchReach Environment Results&lt;/h3&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/fetchreach_random.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/fetchreach_vanillatd3.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/fetchreach_her.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/fetchreach_dense.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best.&quot; /&gt;
        
    &lt;/div&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;The verdict is in: including hindsight experience drastically improved the robot arm’s ability to reach the block! We can see that over 1 million timesteps, the poor sparse TD3 robot arm is unable to learn to reach the block at all. However, with HER, the TD3 + HER robot arm is able to consistently reach the block and achieve close to 0 episode returns within 200k timesteps.&lt;/p&gt;

&lt;div class=&quot;justify-content-center flex&quot;&gt;
    &lt;figure class=&quot;image&quot;&gt;
    &lt;img src=&quot;/img/her/fetchreachrewards.png&quot; alt=&quot;We can see that while Sparse TD3 (blue) fails to learn in this binary reward environment, TD3+HER (green) is able to achieve good performance in under 100k timesteps. It is also the closest to achieving near-zero episode returns out of all the agents, including the TD3 agent trained in a dense reward environment (red). Shaded regions illustrate the standard deviation over 10 evaluation episodes.&quot; /&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;We can see that while Sparse TD3 (blue) fails to learn in this binary reward environment, TD3+HER (green) is able to achieve good performance in under 100k timesteps. It is also the closest to achieving near-zero episode returns out of all the agents, including the TD3 agent trained in a dense reward environment (red). Shaded regions illustrate the standard deviation over 10 evaluation episodes.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
    
    &lt;/figure&gt;
&lt;/div&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Vanilla TD3 Agent &lt;br /&gt; (Sparse Reward)&lt;/th&gt;
      &lt;th&gt;TD3 + HER Agent&lt;br /&gt;(Sparse Reward)&lt;/th&gt;
      &lt;th&gt;Vanilla TD3 Agent &lt;br /&gt;(Dense Reward)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\epsilon\) = 0.05&lt;/td&gt;
      &lt;td&gt;10%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
      &lt;td&gt;5%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p class=&quot;centered&quot;&gt;&lt;i&gt;Using an epsilon of 0.05, the TD3 + HER agent successfully reached the goal 100% of the time. Meanwhile, the vanilla TD3 agent trained in a sparse reward environment only reached the goal 10% of the time.&lt;/i&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, the dense TD3 agent was unable to solve the environment. Its reward may be higher than that of sparse TD3 agent based on the plot above, but playing the recording back shows that it doesn’t appear to perform much better than the sparse TD3 agent.&lt;/p&gt;

&lt;h3 id=&quot;sparsified-reacher-environment-results&quot;&gt;Sparsified Reacher Environment Results&lt;/h3&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/crrandom.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/crsparse.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/crher.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/crdense.gif&quot; alt=&quot;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent.&quot; /&gt;
        
    &lt;/div&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;Although our sparsified environment was more difficult to solve than the FetchReach environment we had initially benchmarked, we were still able to perform far better than the sparse TD3 agent, which you can see above.&lt;/p&gt;

&lt;p&gt;The best settings for the parameters listed in the &lt;a href=&quot;#sparsified-reacher-environment&quot;&gt;Sparsified Reacher Environment section&lt;/a&gt; are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a linearly annealed \(\epsilon\) from 0.07 to 0.05&lt;/li&gt;
  &lt;li&gt;storing a mixture of hindsight and regular experience&lt;/li&gt;
  &lt;li&gt;using a -1/0 binary reward signal based on both distance to goal and action magnitude&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We tracked the returns from our binary reward function, but we also computed the original returns under the environment’s original &lt;strong&gt;shaped reward&lt;/strong&gt; (i.e. non-sparse, non-binary reward) to get a sense of how well our agent performed in the sparsified reacher environment. We can see that although the TD3+HER agent does not perform as well as the dense TD3 agent, it performs far better than the sparse TD3 agent. So, while the agent isn’t able to surpass the original “solved” reward threshold or match the performance in the original environment, it is clearly still able to learn something useful!&lt;/p&gt;

&lt;figure class=&quot;image&quot; style=&quot;width: 80%; margin: auto;&quot;&gt;
    &lt;div class=&quot;image-row&quot;&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/customreacherrewards.png&quot; alt=&quot;We can see that the agent trained on HER + TD3 learns over time with respect to the binary reward function (left). Validating our agent with respect to the original shaped reward function (right) shows that the TD3 + HER agent does, in fact, perform far better than the agent trained on vanilla TD3 with a binary reward function.&quot; /&gt;
        
            &lt;img class=&quot;image&quot; src=&quot;/img/her/customreacheroriginalrewards.png&quot; alt=&quot;We can see that the agent trained on HER + TD3 learns over time with respect to the binary reward function (left). Validating our agent with respect to the original shaped reward function (right) shows that the TD3 + HER agent does, in fact, perform far better than the agent trained on vanilla TD3 with a binary reward function.&quot; /&gt;
        
    &lt;/div&gt;
    &lt;p&gt;&lt;figcaption&gt;&lt;i&gt;We can see that the agent trained on HER + TD3 learns over time with respect to the binary reward function (left). Validating our agent with respect to the original shaped reward function (right) shows that the TD3 + HER agent does, in fact, perform far better than the agent trained on vanilla TD3 with a binary reward function.&lt;/i&gt;&lt;/figcaption&gt;&lt;/p&gt;
&lt;/figure&gt;

&lt;p&gt;As we mentioned earlier, our best-performing TD3+HER agent was trained on an \(\epsilon\) linearly annealed from 0.07 to 0.05. When we benchmarked the final trained agent on our starting epsilon of 0.07, we discovered that it reached the goal 60% of the time. Meanwhile, benchmarking the final trained agent on our ending epsilon of 0.05 demonstrated that the agent reached the goal 50% of the time. Not sure about you, but we’d take those odds over the TD3 agent trained in a sparse reward environment any day of the week!&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;Vanilla TD3 Agent &lt;br /&gt; (Sparse Reward)&lt;/th&gt;
      &lt;th&gt;TD3 + HER Agent&lt;br /&gt;(Sparse Reward)&lt;/th&gt;
      &lt;th&gt;Vanilla TD3 Agent &lt;br /&gt;(Dense Reward)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\epsilon\) = 0.07&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;60%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(\epsilon\) = 0.05&lt;/td&gt;
      &lt;td&gt;0%&lt;/td&gt;
      &lt;td&gt;50%&lt;/td&gt;
      &lt;td&gt;100%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p class=&quot;centered&quot;&gt;&lt;i&gt;Given the starting and ending epsilons we trained on, the TD3 + HER agent was able to reach the goal 50-60% of the time. Meanwhile, the vanilla TD3 agent trained in a sparse reward environment was never able to reach the goal for either epsilon, and the agent trained in the dense reward environment was always able to reach the goal for either epsilon.&lt;/i&gt;&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Varying Epsilon:&lt;/strong&gt; When defining our sparsified Reacher environment, the \(\epsilon\) determining the reward threshold played a large role in the agent’s learning - too large and the agent would too easily be within \(\epsilon\) of the goal and start to jitter around that area. Too small and even the goal-conditioned rewards would be too sparse for a HER agent to learn at all.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; In general (as is the case with any deep learning agent), we had lots of parameters to tune. Many of these (including \(\epsilon\)) arose from the fact that we were trying to modify an existing environment to get a binary reward function; the rest were mostly due to possible variations on HER implementations.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sparsifying the Environment:&lt;/strong&gt; HER was surprisingly effective at reaching the goal in a large percentage of episodes!&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;As our first (but hopefully not last) collab, we were super happy that our modifications to a SOTA algorithm achieved solid performance in binary, sparse-reward environments. We’re particularly excited that even after manually introducing reward sparsity to increase an environment’s difficulty, our agent still learned reasonably well, which confirmed our original hopes that introducing HER would significantly improve the learning achievable by TD3 alone. If you like, you can check out our code &lt;a href=&quot;https://github.com/aimless-agents/TD3&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;! 🤪&lt;/p&gt;

&lt;p&gt;We were initially drawn to goal-conditioned learning because it proves that even though you can’t achieve your loftier goals right now, there is intrinsic value in striving to achieve reachable goals in the present: you might learn something that helps you become successful in the future! Being able to prove this life lesson with an RL agent made this project extra satisfying for us.&lt;/p&gt;

&lt;p&gt;If you have any questions, comments, or doubts, feel free to reach out to us via email, or drop your response below! This is a learning experience for us and we would love to receive feedback on how we could improve.&lt;/p&gt;

&lt;h2 id=&quot;resources&quot;&gt;Resources&lt;/h2&gt;

&lt;p&gt;A huge, HUGE shoutout to Alex LaGrassa and Shreyas Chaudhari, who helped guide us at different parts throughout this entire journey.&lt;/p&gt;

&lt;p&gt;In addition, we referenced a lot of great materials throughout the blogpost and while we were learning more about TD3 and HER! While these lists may not be exhaustive of all of that material, we hope that it is representative of it, and can be of use to someone else.&lt;/p&gt;

&lt;h3 id=&quot;papers&quot;&gt;Papers&lt;/h3&gt;

&lt;p&gt;[1] Marcin Andrychowicz, et al. &lt;a href=&quot;https://arxiv.org/abs/1707.01495&quot; target=&quot;_blank&quot;&gt;Hindsight Experience Replay&lt;/a&gt; (2018).&lt;/p&gt;

&lt;p&gt;[2] Timothy P. Lillicrap, et al. &lt;a href=&quot;https://arxiv.org/abs/1509.02971&quot; target=&quot;_blank&quot;&gt;Continuous control with deep reinforcement learning&lt;/a&gt; (2019).&lt;/p&gt;

&lt;p&gt;[3] Scott Fujimoto, et al. &lt;a href=&quot;https://arxiv.org/abs/1802.09477&quot; target=&quot;_blank&quot;&gt;Addressing Function Approximation Error in Actor-Critic Methods&lt;/a&gt; (2018).&lt;/p&gt;

&lt;p&gt;[4] Matthias Plappert, et al. &lt;a href=&quot;https://arxiv.org/abs/1802.09464&quot; target=&quot;_blank&quot;&gt;Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research&lt;/a&gt; (2018).&lt;/p&gt;

&lt;p&gt;[5] Dhuruva Priyan G M, et al. &lt;a href=&quot;https://arxiv.org/abs/2010.06142&quot; target=&quot;_blank&quot;&gt;Hindsight Experience Replay with Kronecker Product Approximate Curvature&lt;/a&gt;. (2020).&lt;/p&gt;

&lt;h3 id=&quot;other-resources&quot;&gt;Other Resources&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://cmudeeprl.github.io/403_website/&quot; target=&quot;_blank&quot;&gt;Deep RL @ CMU&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93&quot; target=&quot;_blank&quot;&gt;TD3: Learning to Run With AI&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Bcuj2fTH4_4&amp;amp;t=138s&amp;amp;ab_channel=deeplizard&quot; target=&quot;_blank&quot;&gt;Replay Memory Explained&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/paper/hindsight-experience-replay&quot; target=&quot;_blank&quot;&gt;Papers With Code: HER&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/hill-a/stable-baselines&quot; target=&quot;_blank&quot;&gt;Stable Baselines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/sfujim/TD3&quot; target=&quot;_blank&quot;&gt;Fujimoto’s Original TD3 Implementation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gym.openai.com/envs/FetchReach-v1/&quot; target=&quot;_blank&quot;&gt;FetchReach Environment&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://gym.openai.com/envs/Reacher-v2/&quot; target=&quot;_blank&quot;&gt;Reacher Environment&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://openai.com/blog/ingredients-for-robotics-research/&quot; target=&quot;_blank&quot;&gt;OpenAI: Ingredients for Robotics Research&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;

&lt;p&gt;The following is the full set of parameters for our sparsified Reacher experiments we performed:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Varying \(\epsilon\) (the “threshold” maximum distance to goal at which the agent is rewarded).
    &lt;ul&gt;
      &lt;li&gt;We also tried various annealing methods to decrease \(\epsilon\) over time, to try to nudge the agent closer and closer to the goal:
        &lt;ul&gt;
          &lt;li&gt;Linear: \(\epsilon\) decrease by a fixed amount each episode&lt;/li&gt;
          &lt;li&gt;Exponential: \(\epsilon\) decreased according to an exponential curve&lt;/li&gt;
          &lt;li&gt;Step-wise: every x episodes would use a different \(\epsilon\)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using Euclidean distance vs. each element-wise distance to measure goal proximity.&lt;/li&gt;
  &lt;li&gt;Varying reward magnitudes (e.g., 0.1 vs. 1 for positive reward).&lt;/li&gt;
  &lt;li&gt;Instead of positive reward, using a reward “penalty” for not reaching goal (0 reward if target reached, else -1).&lt;/li&gt;
  &lt;li&gt;Varying the \(k\) parameter from the original HER paper, which controls the ratio of HER-selected vs. standard goal-conditioned ER replay memories.&lt;/li&gt;
  &lt;li&gt;Storing both a standard ER and HER-generated memory for each transition (instead of using \(k\) above). In the original paper, for any given episode, \(\frac{k}{k+1}\) transitions would be stored with a HER-selected goal, and the rest are stored with the original intended goal. Here, we also tried storing all transitions with the original goal &lt;em&gt;and&lt;/em&gt; a HER-selected goal.&lt;/li&gt;
  &lt;li&gt;Electricity cost threshold: the agent only gets a reward for reaching the goal &lt;em&gt;and&lt;/em&gt; if it doesn’t exceed this electricity threshold.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s also worth noting that we initially tried sparsifying the &lt;a href=&quot;https://github.com/benelot/pybullet-gym#state-of-implementations&quot; target=&quot;_blank&quot;&gt;Pybullet reimplementation&lt;/a&gt; of MuJoCo Reacher, since we wanted to be able to train without a MuJoCo license. However, we found that the MuJoCo environment was much more amenable to learning over time and ultimately chose to extend the MuJoCo environment instead.&lt;/p&gt;</content><author><name></name></author><category term="reinforcement learning" /><summary type="html">Experiments in goal-conditioned learning.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/%7B%22feature%22=%3E%22her/cover.png%22%7D" /><media:content medium="image" url="http://localhost:4000/%7B%22feature%22=%3E%22her/cover.png%22%7D" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>