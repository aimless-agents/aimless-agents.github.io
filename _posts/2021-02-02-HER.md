---
layout: post
title: "Hindsight is 2020"
excerpt: "Experiments in goal-conditioned learning."
categories: [paragraph, feature photo]
comments: true
image:
---

## Introduction

This past semester we took a <a href="https://cmudeeprl.github.io/403_website/" target="_blank">deep reinforcement learning course</a> together at CMU. The class introduced us to goal-conditioned learning and the Hindsight Experience Replay (HER) algorithm. The underlying concepts of HER (which we’ll describe later) interested us, and we wanted to try reproducing the authors’ results in sparse/binary reward environments (in our case, reacher environments) benchmarked against vanilla TD3. Below, we will outline our approach and explain our results. 

### What is TD3?

{% include image.html url="/img/her-img/td3_spiderman.png" description="An illustration of what TD3 accomplishes with not one, but two critics." size=30 %}

<a href="https://arxiv.org/abs/1802.09477" target="_blank">TD3</a> is an off-policy RL method for continuous action spaces, which improves its predecessor, DDPG, by reducing overestimation bias and improving training stability. This generally results in higher performance than DDPG.

We’re going to assume we all have a working knowledge of RL algorithms, but if you want to learn more then you can read more details about TD3 and DDPG <a href="https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93" target="_blank">here</a>.

### What is HER?

{% include image.html url="https://i.pinimg.com/originals/b7/f6/f4/b7f6f409e68715245296a33ef5452b3c.gif" description="Some good questions from our good man Michael Bluth." size=30 %}

To understand <a href="https://arxiv.org/abs/1707.01495" target="_blank">HER</a>, you’ll first need to understand **regular experience replay (ER)**. ER involves learning from an agent’s memory, by storing past experiences (more specifically, transitions) in some kind of buffer and re-training (replaying) the agent on these transitions.

**Goal-conditioned learning** generally means learning to reach a goal _in addition to_ maximizing reward. Let’s illustrate this with a motivating example:

A soccer player successfully kicks the ball into the goal - this is obviously a helpful learning example, as the player learns to repeat their sequence of actions to achieve their goal. But what about if the ball lands, say, a foot away from the goalpost? We can “move” the goalpost to wherever the ball actually ended up, so the player learns to celebrate getting closer to the originally intended goal rather than feeling sad that they did not make the original goal. This helps the soccer player learn from their mistakes in order to score better.

HER leverages this concept by augmenting real experience collected for the replay buffer with hindsight about states the agent was actually able to reach. For example, let's say Claire is trying to make a goal. If the goal is 3 cells away, traditional goal-conditioned experience replay would tell her to store each of these transitions in her replay buffer with the original goal state at cell 3 and no return (AKA a cumulative reward of 0), since she did not score a goal.


{% include image.html url="/img/her-img/sadsoccerturtle.png" description="Claire tried to score a goal but didn't make it. This makes her feel sad." size=30 %}

But with HER, Claire would also store each of these transitions with a goal state of cell 2 and positive return, since she would have scored had the goal been in cell 2. Even though this is not the original goal we wanted to achieve, we are still able to learn something useful by “moving the goalposts” in hindsight, which is the crux of HER.

{% include image.html url="/img/her-img/happysoccerturtle.png" description="In hindsight, Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!" size=30 %}

HER also helps with sample efficiency: since we augment our experience buffer with these goal-conditioned examples for replay, we have access to far more training data while using the same amount of examples drawn from the environment. 

### Why TD3 + HER?

{% include image.html url="/img/her-img/td3hermovie.png" description="A romance for the epochs." size=50 %}

If you read the HER paper, you’ll notice that the authors implemented HER with DDPG. We were wondering why they didn’t use TD3 instead, given that it’s a marked improvement on DDPG. After some digging, we found out that the HER paper came out one year before Fujimoto published his paper introducing TD3, which was a nice reminder of how quickly the field moves! 

{% include image.html url="/img/her-img/sotagrandma.png" description="It can be difficult to keep up sometimes..." size=50 %}

At any rate, poking around on <a href="https://paperswithcode.com/paper/hindsight-experience-replay" target="_blank">Papers With Code</a> and Google yielded re-implementations of the DDPG + HER results from the original paper, or implementations that introduced <a href="https://arxiv.org/pdf/2010.06142.pdf" target="_blank">other</a> <a href="https://journals.sagepub.com/doi/full/10.1177/1729881419898342" target="_blank">approaches</a> on top of HER + TD3. 

We wanted to see what would happen if we benchmarked vanilla TD3 with hindsight experience replay. We expected that even though TD3 improves stability and reduces overestimation bias during training, it wouldn’t be enough to overcome difficulties in learning with sparse reward. This is where HER comes in.

## Setup

We started with Fujimoto's <a href="https://github.com/sfujim/TD3" target="_blank">original TD3 implementation</a> and added hindsight replay functionality on top of it. For more details, you can see our code <a href="https://github.com/clairecw/TD3" target="_blank">here</a>. 

Our goal was to train the agent in two different variations of a sparse-reward environment: OpenAI’s FetchReach environment (designed for learning sparse reward), and a customization of Mujoco’s Reacher environment (a dense reward environment that we adapted into a sparse reward environment). We will describe the environments in more detail below.

### FetchReach Environment

{% include image.html url="/img/her-img/fetchreach_random.gif" description="Agent in FetchReach environment taking random actions." size=50 %}

There is an <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank">awesome suite of sparse reward environments</a> designed by <a href="https://openai.com/" target="_blank">OpenAI</a> that work with HER already that were benchmarked on <a href="https://arxiv.org/abs/1802.09464" target="_blank">DDPG + HER</a>. The environment we chose to play with was the <a href="https://gym.openai.com/envs/FetchReach-v1/" target="_blank">FetchReach</a> environment. 

This environment is arguably the simplest of this suite of sparse reward environments that OpenAI released. The goal of this environment is to control a 3 degree-of-freedom robotic arm to reach a block placed before it in 3D space. The block’s location is fixed throughout the episode (where an “episode” is one attempt to reach the goal, capped out at some max number of timesteps), but is randomly generated at the start of each episode. 

This environment is default-set to return a sparse reward: it will give a reward of -1 if the block is not within reach of the target, and 0 reward if it is. How does this translate to the robot arm reaching the block? Say that over the 50 timesteps that constitute an episode, the robot arm achieves a cumulative reward, or return, of -50. This means that in none of the timesteps did the robot arm get close to the block, or any goal state. That’s not what we want! However, say that our episode return is 0. This means that over the course of the entire episode, we were close to the block, or one of our goal states, the entire time. That is what we were trying to achieve all along! Therefore, we want to train our TD3 agent to achieve close to 0 episode return, on average.

### Custom Reacher Environment

This environment’s goal is similar to FetchReach, but additionally includes reward penalties for electricity cost (taking too many steps to get to the goal) or getting the robot’s arm joints stuck (the robot arm can no longer move). 
Since we only had individual licenses for MuJoCo and wanted to enable training on AWS clusters, we chose to use the open-source [Pybullet-gym](https://github.com/benelot/pybullet-gym) replica of it.

{% include image.html url="/img/her-img/fetchreach_random.gif" description="TODO: change to be mujoco image taking random actions." size=30 %}

We wanted to convert the Pybullet Reacher environment into an environment with sparse reward. We figured this would more closely resemble a real robot arm trying to reach a target location, since the environment factored in stuck joint cost and electricity cost, which OpenAI’s FetchReach environment doesn’t account for.

 
The main changes we made to the original Bullet environment were:
* **Dense → sparse reward:** the agent only gets a reward if it’s within some distance $$\epsilon$$ of the target location, otherwise 0.
* **Augment the existing standard replay buffer with the agent’s intended goal and goal-conditioned rewards.** We used the HER authors’ “future” goal-choosing scheme, which means after each episode, a state $$s$$ selected for hindsight replay would be stored with its goal being a uniform-randomly selected state that occurred _after_ $$s$$ in the episode.

Some prominent variations to the reward definition and HER training we tried were:
* Varying $$\epsilon$$ (the “threshold” maximum distance to goal at which the agent is rewarded).
* Storing both a standard ER and HER-generated memory for each transition.
* Including/excluding stuck-joint and electricity costs in the reward.

See the Appendix for a full description of these parameters, along with additional things we varied.

## Results

### FetchReach Environment Results

{% include image.html url="/img/her-img/fetchreach_sidebyside.gif" description="On the left, we have the robot arm trained on vanilla TD3, while on the right, we have the robot arm trained on TD3 + HER. Can you spot the difference?" size=30 %}

As you can see, including hindsight experience drastically improved the robot arm’s ability to reach the block. We can see that over 1 million timesteps, the poor robot arm is unable to learn to reach the block with this sparse reward scheme using vanilla TD3. However, with HER, the robot arm is able to consistently reach the block and achieve close to 0 episode returns within 200k timesteps.

{% include image.html url="/img/her-img/fetchreach_her_plot.png" description="We can see that while vanilla TD3 (blue) fails to learn in this binary reward environment, TD3 + HER (green) is able to achieve good performance in under 100k timesteps." size=30 %}

### Custom Reacher Environment Results

pass

## Takeaways

### Varying Epsilon

When defining our custom Reacher environment, the $$\epsilon$$ determining the reward threshold played a large role in the agent’s learning - too large and the agent would easily be within $$\epsilon$$ of the goal, but then just start to jitter around that area (see above for the jitters). Too small and even the goal-conditioned rewards would be too sparse for an agent to learn at all.

### Parameters

In general (as is the case with any deep learning agent), we had lots of parameters to tune. Many of these (including $$\epsilon$$) arose from the fact that we were trying to modify an existing environment to get a binary reward function; the rest were mostly due to possible variations on the existing HER training algorithm. 

### Sparsifying the Environment

With the introduction of real-world considerations like electricity cost and stuck joint cost, this made learning even more difficult after sparsifying the reacher environment. However, HER was surprisingly effective at reaching the goal in a large percentage of episodes!

## Conclusion

As our first (but hopefully not last) collab, we were excited that we were able to reproduce solid results based on modified implementations of SOTA algorithms. 

Check out our code [here](TODO - migrate to joint repo)! 🤪

## Shoutouts

A huge, HUGE shoutout to Alex LaGrassa, Sam Powers, and Shreyas Chaudhari, who were informally our mentors through this entire journey. 

We’d also like to point out some related work done by other lovely folks that you may or may not be interested in checking out, including other implementations of TD3 with HER:
TD3 + HER + KFAC (evaluated only on FetchReach): https://arxiv.org/abs/2010.06142
Stable baselines HER (wraps around DDPG, TD3, SAC, etc): https://stable-baselines.readthedocs.io/en/master/modules/her.html
https://www.semanticscholar.org/paper/Hindsight-policy-gradients-Rauber-Mutz/1fa1f04b80f057e477549e6b9798fab7c7e57db5
https://www.semanticscholar.org/paper/Energy-Based-Hindsight-Experience-Prioritization-Zhao-Tresp/afae13ea5400e74f40e3dcf27afcea5605c83996
HER with experience ranking: https://ieeexplore.ieee.org/document/8850705 

# Appendix


