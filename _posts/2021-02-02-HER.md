---
layout: post
title: "Hindsight is 2020"
excerpt: "Experiments in goal-conditioned learning."
categories: [paragraph, feature photo]
comments: true
image:
  feature: her-img/fetchreach_sidebyside.gif
---

## Introduction

This past semester we took a <a href="https://cmudeeprl.github.io/403_website/" target="_blank">deep reinforcement learning course</a> together at CMU. The class introduced us to goal-conditioned learning and the Hindsight Experience Replay (HER) algorithm. The underlying concepts of HER (which we’ll describe later) interested us, and we wanted to try reproducing the authors’ results in sparse/binary reward environments--with some twists! Below, we will outline our approach and explain our results. 

### What is TD3?

{% include image.html url="/img/her-img/td3_spiderman.png" description="An illustration of what TD3 accomplishes with not one, but two critics." size=30 %}

<a href="https://arxiv.org/abs/1802.09477" target="_blank">TD3</a> is an off-policy RL method for continuous action spaces, which improves its predecessor, DDPG, by reducing overestimation bias and improving training stability. This generally results in higher performance than DDPG.

We’re going to assume we all have a working knowledge of RL algorithms, but if you want to learn more then you can read more details about TD3 and DDPG <a href="https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93" target="_blank">here</a>.

### What is HER?

{% include image.html url="https://i.pinimg.com/originals/b7/f6/f4/b7f6f409e68715245296a33ef5452b3c.gif" description="Some good questions from our good man Michael Bluth." size=30 %}

To understand <a href="https://arxiv.org/abs/1707.01495" target="_blank">HER</a>, you’ll first need to understand **regular experience replay (ER)**. ER involves learning from an agent’s memory, by storing past experiences (more specifically, transitions) in some kind of buffer and re-training (replaying) the agent on these transitions.

**Goal-conditioned learning** generally means learning to reach a goal _in addition to_ maximizing reward. Let’s illustrate this with a motivating example:

A soccer player successfully kicks the ball into the goal - this is obviously a helpful learning example, as the player learns to repeat their sequence of actions to achieve their goal. But what about if the ball lands, say, a foot away from the goalpost? We can “move” the goalpost to wherever the ball actually ended up, so the player learns to celebrate getting closer to the originally intended goal rather than feeling sad that they did not make the original goal. This helps the soccer player learn from their mistakes in order to score better.

HER leverages this concept by augmenting real experience collected for the replay buffer with hindsight about states the agent was actually able to reach. For example, let's say Claire is trying to make a goal. If the goal is 3 cells away, traditional goal-conditioned experience replay would tell her to store each of these transitions in her replay buffer with the original goal state at cell 3 and no return (AKA a cumulative reward of 0), since she did not score a goal.

{% include image.html url="/img/her-img/sadsoccerturtle.png" description="Claire tried to score a goal but didn't make it. This makes her feel sad." size=30 %}

But with HER, Claire would also store each of these transitions with a goal state of cell 2 and positive return, since she would have scored had the goal been in cell 2. Even though this is not the original goal we wanted to achieve, we are still able to learn something useful by “moving the goalposts” in hindsight, which is the crux of HER. 

{% include image.html url="/img/her-img/happysoccerturtle.png" description="In hindsight, Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!" size=30 %}

HER also helps with sample efficiency: since we augment our experience buffer with these goal-conditioned examples for replay, we have access to far more training data while using the same amount of examples drawn from the environment. 

### Why TD3 + HER?

{% include image.html url="/img/her-img/td3hermovie.png" description="A romance for the epochs." size=50 %}

If you read the HER paper, you’ll notice that the authors implemented HER with DDPG. We were wondering why they didn’t use TD3 instead, given that it’s a marked improvement on DDPG. After some digging, we found out that the HER paper came out one year before Fujimoto published his paper introducing TD3, which was a nice reminder of how quickly the field moves! 

{% include image.html url="/img/her-img/sotagrandma.png" description="It can be difficult to keep up sometimes..." size=50 %}

At any rate, poking around on <a href="https://paperswithcode.com/paper/hindsight-experience-replay" target="_blank">Papers With Code</a> and Google yielded re-implementations of the DDPG + HER results from the original paper, or implementations that introduced <a href="https://arxiv.org/pdf/2010.06142.pdf" target="_blank">other</a> <a href="https://journals.sagepub.com/doi/full/10.1177/1729881419898342" target="_blank">approaches</a> on top of HER + TD3. 

We wanted to see what would happen if we benchmarked vanilla TD3 with hindsight experience replay. We expected that even though TD3 improves stability and reduces overestimation bias during training, it wouldn’t be enough to overcome difficulties in learning with sparse reward. This is where HER comes in.

## Setup

We started with Fujimoto's <a href="https://github.com/sfujim/TD3" target="_blank">original TD3 implementation</a> and extended the code to follow the HER training algorithm.
Our goal was then to train the agent in different variations of a sparse-reward environment, which we’ll describe below.

### Implementation

There is an <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank">awesome suite of sparse reward environments</a> designed by <a href="https://openai.com/" target="_blank">OpenAI</a> that work with HER already that were benchmarked on <a href="https://arxiv.org/abs/1802.09464" target="_blank">DDPG + HER</a>. The environment we chose to play with was the <a href="https://gym.openai.com/envs/FetchReach-v1/" target="_blank">FetchReach</a> environment. 

{% include image.html url="/img/her-img/fetchreach_random.gif" description="Agent in FetchReach environment taking random actions." size=50 %}

This environment is arguably the simplest of this suite of sparse reward environments that OpenAI released. The goal of this environment is to control a 3 degree-of-freedom robotic arm to reach a block placed before it in 3D space. The object’s location is fixed throughout the episode (where an “episode” is one attempt to reach the goal, capped out at some max number of timesteps), but is randomly generated at the start of each episode. 

This environment is default-set to return a sparse reward: it will give a reward of -1 if the object is not within reach of the target, and 0 reward if it is. How does this translate to the robot arm reaching the block? Say that over the 50 timesteps that constitute an episode, the robot arm achieves a cumulative reward, or return, of -50. This means that in none of the timesteps did the robot arm get close to the block, or any goal state. That’s not what we want! However, say that our episode return is 0. This means that over the course of the entire episode, we were close to the block, or one of our goal states, the entire time. That is what we were trying to achieve all along! Therefore, we want to train our TD3 agent to achieve close to 0 episode return, on average.

So, armed with Fujimoto’s implementation of TD3 and our implementation of HER, we set out to fetch/reach that block!


