---
layout: post
title: "Hindsight is 2020"
excerpt: "Experiments in goal-conditioned learning."
categories: [paragraph, feature photo]
comments: true
image:
---

* The generated Toc will be an ordered list
{:toc}

## Introduction

This past semester we took a <a href="https://cmudeeprl.github.io/403_website/" target="_blank">deep reinforcement learning course</a> together at CMU. The class introduced us to goal-conditioned learning and  **Hindsight Experience Replay (HER)**. The underlying concepts behind HER interested us, and we wanted to try reproducing the authors’ results in sparse/binary reward environments - in our case, simulated reacher environments - benchmarked against vanilla TD3. We hope to illustrate the benefits of using hindsight experience in sparse, binary reward environments through sharing a discussion of our experimentation and methods.

### What is TD3?

{% include image.html url="/img/her-img/td3_spiderman.png" description="An illustration of how TD3 works." size=30 %}

<a href="https://arxiv.org/abs/1802.09477" target="_blank">TD3</a> is an off-policy RL method for continuous action spaces, which improves its predecessor, DDPG, by reducing overestimation bias and improving training stability. This generally results in higher performance than DDPG. We’re going to assume we all have a working knowledge of RL algorithms, but if you want to learn more then you can read more details about TD3 and DDPG <a href="https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93" target="_blank">here</a>.

### What is HER?

{% include image.html url="https://i.pinimg.com/originals/b7/f6/f4/b7f6f409e68715245296a33ef5452b3c.gif" description="Some good questions from our good man Michael Bluth." size=30 %}

To understand <a href="https://arxiv.org/abs/1707.01495" target="_blank">HER</a>, you’ll first need to understand **regular experience replay (ER)**. ER involves learning from an agent’s memory, by storing past experiences (more specifically, transitions) in a replay buffer. Then, the agent trains on randomly sampled transitions from the replay buffer by "replaying" these transitions. Training on randomly sampled transitions rather than sequentially collected experience helps de-correlate training data and improves learning. You can learn more about it <a href="https://www.youtube.com/watch?v=Bcuj2fTH4_4&t=138s&ab_channel=deeplizard" target="_blank">here</a>.

**Goal-conditioned learning** generally means learning to reach a goal _in addition to_ maximizing reward. Let’s illustrate this with a motivating example:

A soccer player successfully kicks the ball into the goal - this is obviously a helpful learning example, as the player learns to repeat their sequence of actions to achieve their goal. But what about if the ball lands, say, a foot away from the goalpost? We can “move” the goalpost to wherever the ball actually ended up, so the player learns to celebrate getting closer to the originally intended goal rather than feeling sad that they did not make the original goal. This helps the soccer player learn from their mistakes in order to score better.

HER leverages this concept by augmenting real experience collected for the replay buffer with hindsight about states the agent was actually able to reach. For example, let's say Turtle Claire is trying to make a goal. If the goal is 3 cells away, traditional goal-conditioned experience replay would tell her to store each of these transitions in her replay buffer with the original goal state at cell 3 and no return (AKA a cumulative reward of 0), since she did not score a goal.

{% include image.html url="/img/her-img/sadsoccerturtle.png" description="Turtle Claire tried to score a goal but didn't make it. This makes her feel sad." size=30 %}

But with HER, Turtle Claire would also store each of these transitions with a goal state of cell 2 and positive return, since she would have scored had the goal been in cell 2. Even though this is not the original goal we wanted to achieve, we are still able to learn something useful by “moving the goalposts” in hindsight, which is the crux of HER.

{% include image.html url="/img/her-img/happysoccerturtle.png" description="In hindsight, Turtle Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!" size=30 %}

HER and goal-conditioned methods aim to solve the issue of learning in an environment with **sparse rewards**, which means very few states/actions in the environment actually give a positive or non-zero reward signal. Often, these sparse rewards are **binary**, as in Turtle Claire's soccer scenario: you have either scored a goal, or you have not. 

HER also helps with **sample efficiency**: since we augment our experience buffer with these goal-conditioned examples for replay, we have access to far more training data while using the same amount of examples drawn from the environment. 


### Why TD3 + HER?

{% include image.html url="/img/her-img/td3hermovie.png" description="A romance for the epochs." size=50 %}

If you read the HER paper, you’ll notice that the authors implemented HER with DDPG. We were wondering why they didn’t use TD3 instead, given that it’s a marked improvement on DDPG. After some digging, we found out that the HER paper came out one year before Fujimoto published his paper introducing TD3, which was a nice reminder of how quickly the field moves! 

{% include image.html url="/img/her-img/sotagrandma.png" description="It can be difficult to keep up sometimes..." size=30 %}

At any rate, poking around on <a href="https://paperswithcode.com/paper/hindsight-experience-replay" target="_blank">Papers With Code</a> and Google yielded re-implementations of the DDPG + HER results from the original paper, or implementations that introduced <a href="https://arxiv.org/pdf/2010.06142.pdf" target="_blank">other</a> <a href="https://journals.sagepub.com/doi/full/10.1177/1729881419898342" target="_blank">approaches</a> on top of HER + TD3. Of course, there's always <a href="https://github.com/hill-a/stable-baselines" target="_blank">stable baselines</a>, but we really wanted to explore and implement HER and evaluate the results ourselves with respect to TD3. 

We wanted to see what would happen if we trained our agents using TD3 with HER, instead of standard ER (which TD3 was originally proposed with). Even though TD3 tends to perform better than DDPG by improving stability and reducing overestimation bias during training, we expect that it won't on its own to overcome difficulties in learning with sparse reward. This is where HER comes in.

## Setup

We started with Fujimoto's <a href="https://github.com/sfujim/TD3" target="_blank">original TD3 implementation</a> and added hindsight replay functionality on top of it. For more details, you can see our code <a href="https://github.com/clairecw/TD3" target="_blank">here</a>. 

Our goal was to train the agent in two different variations of a sparse-reward environment: OpenAI’s <a href="https://gym.openai.com/envs/FetchReach-v1/" target="_blank">FetchReach environment</a> (designed for learning sparse reward), and a customization of OpenAI's <a href="https://gym.openai.com/envs/Reacher-v2/" target="_blank"> MuJoCo Reacher environment</a> (a dense reward environment that we adapted into a sparse reward environment). We will describe the environments in more detail below.

### FetchReach Environment

{% include image.html url="/img/her-img/fetchreach_random.gif" description="Agent in FetchReach environment taking random actions." size=30 %}

There is an <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank">awesome suite of sparse reward environments</a> designed by <a href="https://openai.com/" target="_blank">OpenAI</a> that work with HER already that were benchmarked on <a href="https://arxiv.org/abs/1802.09464" target="_blank">DDPG + HER</a>.

This environment is arguably the simplest of this suite of sparse reward environments that OpenAI released. The goal of this environment is to control a 3 degree-of-freedom robotic arm to reach a block placed before it in 3D space. The block’s location is fixed throughout the episode (where an “episode” is one attempt to reach the goal, capped out at some max number of timesteps), but is randomly generated at the start of each episode. 

This environment is default-set to return a sparse reward: it will give a reward of -1 if the block is not within reach of the target, and 0 reward if it is. How does this translate to the robot arm reaching the block? Say that over the 50 timesteps that constitute an episode, the robot arm achieves a cumulative reward, or return, of -50. This means that in none of the timesteps did the robot arm get close to the block, or any goal state. That’s not what we want! However, say that our episode return is 0. This means that over the course of the entire episode, we were close to the block, or one of our goal states, the entire time. That is what we were trying to achieve all along! Therefore, we want to train our TD3 agent to achieve close to 0 episode return, on average.

### Custom Reacher Environment

{% include image.html url="/img/her-img/crrandom.gif" description="Agent in Reacher environment taking random actions." size=30 %}

The Reacher-v2 OpenAI Gym environment is similar to FetchReach in that a robot arm must reach a goal object. However, it has a shaped reward function that includes distance to the goal object, as well as a reward penalty for extraneous robot arm movement, and only has two degrees of freedom. In addition, this environment is considered “solved” (i.e. the robot arm has reached the goal object) when the episode return is greater than -3.75. 

OpenAI’s FetchReach environment was made _specifically_ to benchmark goal-conditioned learning strategies, so we wanted to challenge our implementation to see if it could still solve an environment that wasn’t “born” to be sparse. Therefore, we figured that the Reacher-v2 OpenAI Gym environment, which has a similar goal to the FetchReach environment but was designed with a shaped reward signal in mind, would be a good candidate for sparsification.

{% include image.html url="/img/her-img/borntobesparse.png" description="Some environments were born to be sparse." size=30 %}
 
The main changes we made to the original Bullet environment were:
* **Sparsifying the environment:** the agent only gets a reward if it’s within some distance $$\epsilon$$ of the target location, otherwise 0.
* **Augment the existing standard replay buffer with the agent’s intended goal and goal-conditioned rewards.** We used the HER authors’ “future” goal-choosing scheme, which means after each episode, a state $$s$$ selected for hindsight replay would be stored with its goal being a uniform-randomly selected state that occurred _after_ $$s$$ in the episode.

Some prominent variations to the reward definition and HER training we tried were:
* Varying $$\epsilon$$, or how "off" the agent can be from the goal and still receive reward for "achieving" the goal.
* Varying $$k$$, the ratio of standard ER and HER-generated memory to store for replay. 
* Including/excluding factors other than distance to goal in the reward function.

See the Appendix for a full description of these parameters, along with additional things we varied.

## Results

### FetchReach Environment Results

{% include image_set_single_caption.html urls="/img/her-img/fetchreach_random.gif, /img/her-img/fetchreach_vanillatd3.gif, /img/her-img/fetchreach_her.gif, /img/her-img/fetchreach_dense.gif" description="From left to right: random agent, TD3 agent in a sparse reward environment, TD3 + HER agent trained in a sparse reward environment, and TD3 agent in a dense reward environment. Clearly, the TD3+HER agent (3rd agent from the left) performs the best." size=30 %}

The verdict is in: including hindsight experience drastically improved the robot arm’s ability to reach the block! We can see that over 1 million timesteps, the poor robot arm is unable to learn to reach the block with this sparse reward scheme using vanilla TD3. However, with HER, the robot arm is able to consistently reach the block and achieve close to 0 episode returns within 200k timesteps.

{% include image.html url="/img/her-img/fetchreachrewards.png" description="We can see that while vanilla TD3 (blue) fails to learn in this binary reward environment, TD3 + HER (green) is able to achieve good performance in under 100k timesteps. It is also the closest to achieving near-zero episode returns out of all the agents, including the TD3 agent trained in a dense reward environment (red). " size=30 %}

Interestingly, the TD3 agent trained in a dense reward environment was unable to solve the environment. Its reward may be higher than that of TD3 trained on sparse reward based on the plot above, but playing the recording back shows that it doesn’t appear to perform much better than the TD3 agent trained in the sparse reward environment.  

### Custom Reacher Environment Results

{% include image_set_single_caption.html urls="/img/her-img/crrandom.gif, /img/her-img/crsparse.gif, /img/her-img/crher.gif, /img/her-img/crdense.gif" description="From left to right: random agent, TD3 agent in a sparse reward environment, TD3 + HER agent trained in a sparse reward environment, and TD3 agent in a dense reward environment. Although the TD3 agent trained in a dense reward environment (4th agent from the left) performs the best, the TD3+HER agent performs significantly better than random." size=30 %}

Although our custom environment was more difficult to solve than the FetchReach environment we had initially benchmarked, we were still able to perform far better than vanilla TD3. The best settings for the parameters listed above are: 

* a linearly annealed $$\epsilon$$ from 0.07 → 0.05 
* storing a mixture of hindsight and regular experience
* using a -1/0 binary reward signal based on both distance to goal and action magnitude


We tracked the returns from our binary reward function, but we also computed the original returns under the environment’s original shaped reward to get a sense of how well our agent performed in the custom environment. We can see that although the TD3 + HER agent does not perform as well as the TD3 agent trained with dense rewards, it performs far better than the vanilla TD3 agent in our constructed sparse reward environment. So, while the agent isn’t able to surpass the original “solved” reward threshold or match the performance in the original environment, it is clearly still able to learn something! 

{% include image_set_single_caption.html urls="/img/her-img/customreacherrewards.png, /img/her-img/customreacheroriginalrewards.png" description="We can see that the agent trained on HER + TD3 learns over time with respect to the binary reward function (left). Validating our agent with respect to the original shaped reward function (right) shows that the TD3 + HER agent does, in fact, perform far better than the agent trained on vanilla TD3 with a binary reward function." size=40 %}

As we mentioned, our best-performing TD3 + HER agent was trained on an $$\epsilon$$ linearly annealed from 0.07 to 0.05. When we benchmarked the final trained agent on our starting epsilon of 0.07, we discovered that it reached the goal 60% of the time. Meanwhile, benchmarking the final trained agent on our ending epsilon of 0.05 demonstrated that the agent reached the goal 50% of the time. I don’t know about you, but I’d take those odds over the TD3 agent trained in a sparse reward environment any day of the week!

{% include image.html url="/img/her-img/crchart.png" description="Given the starting and ending epsilons we trained on, the TD3 + HER agent was able to reach the goal 50-60% of the time. Meanwhile, the vanilla TD3 agent trained in a sparse reward environment was never able to reach the goal for either epsilon, and the agent trained in the dense reward environment was always able to reach the goal for either epsilon." size=50 %}

## Takeaways

* **Varying Epsilon:** When defining our custom Reacher environment, the $$\epsilon$$ determining the reward threshold played a large role in the agent’s learning - too large and the agent would easily be within $$\epsilon$$ of the goal, but then just start to jitter around that area (see above for the jitters). Too small and even the goal-conditioned rewards would be too sparse for an agent to learn at all.

* **Parameters:** In general (as is the case with any deep learning agent), we had lots of parameters to tune. Many of these (including $$\epsilon$$) arose from the fact that we were trying to modify an existing environment to get a binary reward function; the rest were mostly due to possible variations on the existing HER training algorithm. 

* **Sparsifying the Environment** With the introduction of real-world considerations like electricity cost and stuck joint cost, this made learning even more difficult after sparsifying the reacher environment. However, HER was surprisingly effective at reaching the goal in a large percentage of episodes!

## Conclusion

As our first (but hopefully not last) collab, we were excited that we were able to reproduce solid results based on modified implementations of SOTA algorithms. 

Check out our code [here](TODO - migrate to joint repo)! 🤪

## Shoutouts

A huge, HUGE shoutout to Alex LaGrassa, Sam Powers, and Shreyas Chaudhari, who were informally our mentors through this entire journey. 

We’d also like to point out some related work done by other lovely folks that you may or may not be interested in checking out, including other implementations of TD3 with HER:
TD3 + HER + KFAC (evaluated only on FetchReach): https://arxiv.org/abs/2010.06142
Stable baselines HER (wraps around DDPG, TD3, SAC, etc): https://stable-baselines.readthedocs.io/en/master/modules/her.html
https://www.semanticscholar.org/paper/Hindsight-policy-gradients-Rauber-Mutz/1fa1f04b80f057e477549e6b9798fab7c7e57db5
https://www.semanticscholar.org/paper/Energy-Based-Hindsight-Experience-Prioritization-Zhao-Tresp/afae13ea5400e74f40e3dcf27afcea5605c83996
HER with experience ranking: https://ieeexplore.ieee.org/document/8850705 

# Appendix


