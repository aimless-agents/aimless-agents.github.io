---
layout: post
title: "Hindsight is 2020"
excerpt: "Experiments in goal-conditioned learning."
categories: [reinforcement learning]
comments: true
image:
---

* The generated Toc will be an ordered list
{:toc}

## Introduction

This past semester we took a <a href="https://cmudeeprl.github.io/403_website/" target="_blank">deep reinforcement learning course</a> together at CMU. The class introduced us to goal-conditioned learning and  **Hindsight Experience Replay (HER)**. The underlying concepts behind HER interested us, and we wanted to try reproducing the authors‚Äô results in sparse/binary reward environments - in our case, simulated reacher environments - benchmarked against vanilla TD3. We hope to illustrate the benefits of using hindsight experience in sparse, binary reward environments through sharing a discussion of our experimentation and methods. While we won't go in-depth into the implementation details, we hope that you come out of this with a better idea of why hindsight replay is so useful and interesting, and how we went about trying to prove it!

### What is TD3?

{% include image.html url="/img/her-img/td3_spiderman.png" description="An illustration of how TD3 works." size=30 %}

<a href="https://arxiv.org/abs/1802.09477" target="_blank">TD3</a> is an off-policy RL method for continuous action spaces, which improves its predecessor, DDPG, by reducing overestimation bias and improving training stability. This generally results in higher performance than DDPG. We‚Äôre going to assume we all have a working knowledge of RL algorithms, but if you want to learn more then you can read more details about TD3 and DDPG <a href="https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93" target="_blank">here</a>.

### What is HER?

{% include image.html url="https://i.pinimg.com/originals/b7/f6/f4/b7f6f409e68715245296a33ef5452b3c.gif" description="Some good questions from our good man Michael Bluth." size=30 %}

To understand <a href="https://arxiv.org/abs/1707.01495" target="_blank">HER</a>, you‚Äôll first need to understand **regular experience replay (ER)**. ER involves learning from an agent‚Äôs memory, by storing past experiences (more specifically, transitions) in a replay buffer. Then, the agent trains on randomly sampled transitions from the replay buffer by "replaying" these transitions. Training on randomly sampled transitions rather than sequentially collected experience helps de-correlate training data and improves learning. You can learn more about it <a href="https://www.youtube.com/watch?v=Bcuj2fTH4_4&t=138s&ab_channel=deeplizard" target="_blank">here</a>.

**Goal-conditioned learning** generally means learning to reach a goal _in addition to_ maximizing reward. Let‚Äôs illustrate this with a motivating example:

A soccer player successfully kicks the ball into the goal - this is obviously a helpful learning example, as the player learns to repeat their sequence of actions to achieve their goal. But what about if the ball lands, say, a foot away from the goalpost? We can ‚Äúmove‚Äù the goalpost to wherever the ball actually ended up, so the player learns to celebrate making goals rather than feeling sad that they did not make the original goal. This helps the soccer player learn how to score better overall.

HER leverages this concept by augmenting real experience collected for the replay buffer with hindsight about states the agent was actually able to reach. For example, let's say Turtle Claire is trying to make a goal. If the goal is 3 cells away, traditional goal-conditioned experience replay would tell her to store each of these transitions in her replay buffer with the original goal state at cell 3 and no return (AKA a cumulative reward of 0), since she did not score a goal.

{% include image.html url="/img/her-img/sadsoccerturtle.png" description="Turtle Claire tried to score a goal but didn't make it. This makes her feel sad." size=30 %}

But with HER, Turtle Claire would also store each of these transitions with a goal state of cell 2 and positive return, since she would have scored had the goal been in cell 2. Even though this is not the original goal we wanted to achieve, we are still able to learn something useful by ‚Äúmoving the goalposts‚Äù in hindsight, which is the crux of HER.

{% include image.html url="/img/her-img/happysoccerturtle.png" description="In hindsight, Turtle Claire knows that if the goal had been in cell 2, then she would have made a goal. This makes her happier!" size=30 %}

HER and goal-conditioned methods aim to solve the issue of learning in an environment with **sparse rewards**, which means very few states/actions in the environment actually give a positive or non-zero reward signal. Often, these sparse rewards are **binary**, as in Turtle Claire's soccer scenario: you have either scored a goal, or you have not. 

HER also helps with **sample efficiency**: since we augment our experience buffer with these goal-conditioned examples for replay, we have access to far more training data while using the same amount of examples drawn from the environment. 


### Why TD3 + HER?

{% include image.html url="/img/her-img/td3hermovie.png" description="A romance for the epochs." size=50 %}

If you read the HER paper, you‚Äôll notice that the authors implemented HER with DDPG. We were wondering why they didn‚Äôt use TD3 instead, given that it‚Äôs a marked improvement on DDPG. After some digging, we found out that the HER paper came out one year before Fujimoto published his paper introducing TD3, which was a nice reminder of how quickly the field moves! 

{% include image.html url="/img/her-img/sotagrandma.png" description="It can be difficult to keep up sometimes..." size=30 %}

At any rate, poking around on <a href="https://paperswithcode.com/paper/hindsight-experience-replay" target="_blank">Papers With Code</a> and Google yielded re-implementations of the DDPG + HER results from the original paper. Although we did find some implementations of TD3+HER out there (like <a href="https://arxiv.org/pdf/2010.06142.pdf" target="_blank">this</a>, <a href="https://journals.sagepub.com/doi/full/10.1177/1729881419898342" target="_blank">this</a>, and <a href="https://github.com/hill-a/stable-baselines" target="_blank">this</a>), our curiosity was sufficiently piqued by TD3 and HER that we wanted to do our own exploration of it. 

Specifically, we wanted to see what would happen if we trained our agents using TD3 with HER, instead of standard ER (which TD3 was originally proposed with). Even though TD3 tends to perform better than DDPG by improving stability and reducing overestimation bias during training, we expect that it won't be able to overcome difficulties with learning in sparse reward environments. This is where HER comes in.

## Setup

We started with Fujimoto's <a href="https://github.com/sfujim/TD3" target="_blank">original TD3 implementation</a> and added hindsight replay functionality on top of it. For more details, you can see our code <a href="https://github.com/clairecw/TD3" target="_blank">here</a>. 

Our goal was to train the agent in two different variations of a sparse-reward environment: OpenAI‚Äôs <a href="https://gym.openai.com/envs/FetchReach-v1/" target="_blank">FetchReach environment</a> (designed for learning sparse reward), and a customization of OpenAI's <a href="https://gym.openai.com/envs/Reacher-v2/" target="_blank"> MuJoCo Reacher environment</a> (a dense reward environment that we adapted into a sparse reward environment). 

For both environments, we evaluated the corresponding robot arm's performance trained on:

* vanilla TD3 in a sparse reward environment, which we will refer to as the ***Sparse TD3*** agent
* TD3 + HER in a sparse reward environment, which we will refer to as the ***TD3+HER*** agent
* vanilla TD3 in a dense reward environment, which we will refer to as the ***Dense TD3*** agent

We will describe the environments in more detail below.

### FetchReach Environment

{% include image.html url="/img/her-img/fetchreach_random.gif" description="Agent in FetchReach environment taking random actions." size=30 %}

There is an <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank">awesome suite of sparse reward environments</a> designed by <a href="https://openai.com/" target="_blank">OpenAI</a> that work with HER already that were benchmarked on <a href="https://arxiv.org/abs/1802.09464" target="_blank">DDPG + HER</a>.

This environment is arguably the simplest of this suite of sparse reward environments that OpenAI released. The goal of this environment is to control a 3 degree-of-freedom robotic arm to reach a block placed before it in 3D space. The block‚Äôs location is fixed throughout the episode (where an ‚Äúepisode‚Äù is one attempt to reach the goal, capped out at some max number of timesteps), but is randomly generated at the start of each episode. 

This environment is default-set to return a sparse reward: it will give a reward of -1 if the block is not within reach of the target, and 0 reward if it is. How does this translate to the robot arm reaching the block? Say that over the 50 timesteps that constitute an episode, the robot arm achieves a cumulative reward, or return, of -50. This means that in none of the timesteps did the robot arm get close to the block, or any goal state. That‚Äôs not what we want! However, say that our episode return is 0. This means that over the course of the entire episode, we were close to the block, or one of our goal states, the entire time. That is what we were trying to achieve all along! Therefore, we want to train our TD3 agent to achieve close to 0 episode return, on average.

### Custom Reacher Environment

{% include image.html url="/img/her-img/crrandom.gif" description="Agent in Reacher environment taking random actions." size=30 %}

The Reacher-v2 OpenAI Gym environment is similar to FetchReach in that a robot arm must reach a goal object. However, it has a shaped reward function that includes distance to the goal object, as well as a reward penalty for extraneous robot arm movement, and only has two degrees of freedom. In addition, this environment is considered ‚Äúsolved‚Äù (i.e. the robot arm has reached the goal object) when the episode return is greater than -3.75. 

OpenAI‚Äôs FetchReach environment was made _specifically_ to benchmark goal-conditioned learning strategies, so we wanted to challenge our implementation to see if it could still solve an environment that wasn‚Äôt ‚Äúborn‚Äù to be sparse. Therefore, we figured that the Reacher-v2 OpenAI Gym environment, which has a similar goal to the FetchReach environment but was designed with a shaped reward signal in mind, would be a good candidate for sparsification.

{% include image.html url="/img/her-img/borntobesparse.png" description="Some environments were born to be sparse." size=30 %}
 
The main changes we made to the original Bullet environment were:
* **Sparsifying the environment:** the agent only gets a reward if it‚Äôs within some distance $$\epsilon$$ of the target location, otherwise 0.
* **Augment the existing standard replay buffer with the agent‚Äôs intended goal and goal-conditioned rewards.** We used the HER authors‚Äô ‚Äúfuture‚Äù goal-choosing scheme, which means after each episode, a state $$s$$ selected for hindsight replay would be stored with its goal being a uniform-randomly selected state that occurred _after_ $$s$$ in the episode.

Some prominent variations to the reward definition and HER training we tried were:
* Varying $$\epsilon$$, or how "off" the agent can be from the goal and still receive reward for "achieving" the goal.
* Varying $$k$$, the ratio of standard ER and HER-generated memory to store for replay. 
* Including/excluding factors other than distance to goal in the reward function.

See the Appendix for a full description of these parameters, along with additional things we varied.

## Results

### FetchReach Environment Results

{% include image_set_single_caption.html urls="/img/her-img/fetchreach_random.gif, /img/her-img/fetchreach_vanillatd3.gif, /img/her-img/fetchreach_her.gif, /img/her-img/fetchreach_dense.gif" description="From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Clearly, the TD3+HER agent (3rd agent from the left) performs the best." size=30 %}

The verdict is in: including hindsight experience drastically improved the robot arm‚Äôs ability to reach the block! We can see that over 1 million timesteps, the poor sparse TD3 robot arm is unable to learn to reach the block at all. However, with HER, the TD3 + HER robot arm is able to consistently reach the block and achieve close to 0 episode returns within 200k timesteps.

{% include image.html url="/img/her-img/fetchreachrewards.png" description="We can see that while Sparse TD3 (blue) fails to learn in this binary reward environment, TD3+HER (green) is able to achieve good performance in under 100k timesteps. It is also the closest to achieving near-zero episode returns out of all the agents, including the TD3 agent trained in a dense reward environment (red). " size=30 %}

Interestingly, the dense TD3 agent was unable to solve the environment. Its reward may be higher than that of sparse TD3 agent based on the plot above, but playing the recording back shows that it doesn‚Äôt appear to perform much better than the sparse TD3 agent.  

### Custom Reacher Environment Results

{% include image_set_single_caption.html urls="/img/her-img/crrandom.gif, /img/her-img/crsparse.gif, /img/her-img/crher.gif, /img/her-img/crdense.gif" description="From left to right: random agent, sparse TD3 agent, TD3+HER agent, and dense TD3 agent. Although the dense TD3 agent (rightmost) performs the best, the TD3+HER agent performs significantly better than the random agent." size=30 %}

Although our custom environment was more difficult to solve than the FetchReach environment we had initially benchmarked, we were still able to perform far better than the sparse TD3 agent, which you can see above. 

The best settings for the parameters listed in the <a href="#custom-reacher-environment">Custom Reacher Environment section</a> are: 

* a linearly annealed $$\epsilon$$ from 0.07 ‚Üí 0.05 
* storing a mixture of hindsight and regular experience
* using a -1/0 binary reward signal based on both distance to goal and action magnitude


We tracked the returns from our binary reward function, but we also computed the original returns under the environment‚Äôs original shaped reward to get a sense of how well our agent performed in the custom environment. We can see that although the TD3+HER agent does not perform as well as the dense TD3 agent, it performs far better than the sparse TD3 agent. So, while the agent isn‚Äôt able to surpass the original ‚Äúsolved‚Äù reward threshold or match the performance in the original environment, it is clearly still able to learn something useful! 

{% include image_set_single_caption.html urls="/img/her-img/customreacherrewards.png, /img/her-img/customreacheroriginalrewards.png" description="We can see that the agent trained on HER + TD3 learns over time with respect to the binary reward function (left). Validating our agent with respect to the original shaped reward function (right) shows that the TD3 + HER agent does, in fact, perform far better than the agent trained on vanilla TD3 with a binary reward function." size=40 %}

As we mentioned earlier, our best-performing TD3+HER agent was trained on an $$\epsilon$$ linearly annealed from 0.07 to 0.05. When we benchmarked the final trained agent on our starting epsilon of 0.07, we discovered that it reached the goal 60% of the time. Meanwhile, benchmarking the final trained agent on our ending epsilon of 0.05 demonstrated that the agent reached the goal 50% of the time. I don‚Äôt know about you, but I‚Äôd take those odds over the TD3 agent trained in a sparse reward environment any day of the week!

{% include image.html url="/img/her-img/crchart.png" description="Given the starting and ending epsilons we trained on, the TD3 + HER agent was able to reach the goal 50-60% of the time. Meanwhile, the vanilla TD3 agent trained in a sparse reward environment was never able to reach the goal for either epsilon, and the agent trained in the dense reward environment was always able to reach the goal for either epsilon." size=50 %}

## Takeaways

* **Varying Epsilon:** When defining our custom Reacher environment, the $$\epsilon$$ determining the reward threshold played a large role in the agent‚Äôs learning - too large and the agent would too easily be within $$\epsilon$$ of the goal and start to jitter around that area. Too small and even the goal-conditioned rewards would be too sparse for a HER agent to learn at all.

* **Parameters:** In general (as is the case with any deep learning agent), we had lots of parameters to tune. Many of these (including $$\epsilon$$) arose from the fact that we were trying to modify an existing environment to get a binary reward function; the rest were mostly due to possible variations on HER implementations. 

* **Sparsifying the Environment** HER was surprisingly effective at reaching the goal in a large percentage of episodes!

## Conclusion

As our first (but hopefully not last) collab, we were excited that we were able to reproduce solid results based on modified implementations of SOTA algorithms. 

Check out our code [here](TODO - migrate to joint repo)! ü§™

# Appendix

## Resources

A huge, HUGE shoutout to Alex LaGrassa, Sam Powers, and Shreyas Chaudhari, who helped guide us at different parts throughout this entire journey. 

In addition, we referenced a lot of great material throughout the blogpost and while we were learning more about TD3 and HER! While this list may not be exhaustive of all of that material, we hope that it is representative of it, and can be of use to someone else.

### Papers

[1] Marcin Andrychowicz, et al. <a href="https://arxiv.org/abs/1707.01495" target="_blank">Hindsight Experience Replay</a>.(2018).

[2] Timothy P. Lillicrap, et al. <a href="https://arxiv.org/abs/1509.02971" target="_blank">Continuous control with deep reinforcement learning</a>. (2019).

[3] Scott Fujimoto, et al. <a href="https://arxiv.org/abs/1802.09477" target="_blank">Addressing Function Approximation Error in Actor-Critic Methods</a>. (2018).

[4] Matthias Plappert, et al. <a href="https://arxiv.org/abs/1802.09464" target="_blank">Multi-Goal Reinforcement Learning: Challenging Robotics Environments and Request for Research</a> (2018).

[5] Dhuruva Priyan G M, et al. <a href="https://arxiv.org/abs/2010.06142" target="_blank">Hindsight Experience Replay with Kronecker Product Approximate Curvature</a>. (2020).

### Other Resources

* <a href="https://cmudeeprl.github.io/403_website/" target="_blank">Deep RL @ CMU</a>

* <a href="https://towardsdatascience.com/td3-learning-to-run-with-ai-40dfc512f93" target="_blank">TD3: Learning to Run With AI</a>

* <a href="https://www.youtube.com/watch?v=Bcuj2fTH4_4&t=138s&ab_channel=deeplizard" target="_blank">Replay Memory Explained</a>

* <a href="https://paperswithcode.com/paper/hindsight-experience-replay" target="_blank">Papers With Code: HER</a>

* <a href="https://github.com/hill-a/stable-baselines" target="_blank">Stable Baselines</a>

* <a href="https://github.com/sfujim/TD3" target="_blank">Fujimoto's Original TD3 Implementation</a>

* <a href="https://gym.openai.com/envs/FetchReach-v1/" target="_blank">FetchReach Environment</a>

* <a href="https://gym.openai.com/envs/Reacher-v2/" target="_blank">Reacher Environment</a>

* <a href="https://openai.com/blog/ingredients-for-robotics-research/" target="_blank">OpenAI: Ingredients for Robotics Research</a>




